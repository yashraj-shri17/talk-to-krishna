"""
LLM Integration module for generating contextual answers using Groq's Llama 3.1.
"""
from typing import List, Dict, Any, Optional
from groq import Groq
from src.config import settings
from src.logger import setup_logger

logger = setup_logger(__name__, settings.LOG_LEVEL, settings.LOG_FILE)


class LLMAnswerGenerator:
    """Generate contextual answers using LLM based on retrieved shlokas."""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or settings.GROQ_API_KEY
        if not self.api_key:
            logger.warning("GROQ_API_KEY not set.")
            self.client = None
        else:
            try:
                self.client = Groq(api_key=self.api_key)
            except Exception as e:
                logger.error(f"Groq init failed: {e}")
                self.client = None
        
        self.model = settings.LLM_MODEL
    
    def is_available(self) -> bool:
        return self.client is not None
    
    def format_shlokas_for_context(self, shlokas: List[Dict[str, Any]]) -> str:
        context_parts = []
        for i, shloka in enumerate(shlokas, 1):
            # Use English meaning for LLM context (better understanding)
            # But keep Sanskrit for reference
            english_meaning = shloka.get('meaning_english', shloka.get('meaning', ''))
            
            context_parts.append(
                f"Shloka ID: {shloka.get('id')}\n"
                f"Sanskrit: {shloka.get('sanskrit')}\n"
                f"Meaning: {english_meaning}\n"
            )
        return "\n".join(context_parts)
    
    def format_conversation_history(self, history: List[Dict[str, Any]]) -> str:
        """Format conversation history for LLM context."""
        if not history:
            return ""
        
        formatted = ["‡§™‡§ø‡§õ‡§≤‡•Ä ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:"]
        for i, conv in enumerate(history[-3:], 1):  # Last 3 conversations
            formatted.append(f"{i}. ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {conv['question']}")
            formatted.append(f"   ‡§â‡§§‡•ç‡§§‡§∞: {conv['answer'][:100]}...")
        
        return "\n".join(formatted)
    
    def generate_answer(
        self,
        user_question: str,
        retrieved_shlokas: List[Dict[str, Any]],
        conversation_history: List[Dict[str, Any]] = None,
        max_tokens: int = 600,  # Increased for follow-up questions
        temperature: float = 0.3,  # Slightly higher for more natural follow-ups
        stream: bool = True  # Enable streaming for faster perceived latency
    ) -> Dict[str, Any]:
        
        if not self.is_available():
            return {'answer': None, 'shlokas': retrieved_shlokas, 'llm_used': False}
        
        try:
            context = self.format_shlokas_for_context(retrieved_shlokas)
            history_context = self.format_conversation_history(conversation_history or [])
            
            # ENHANCED PROMPT with Strict Concise Formatting
            system_prompt = """‡§§‡•Å‡§Æ ‡§≠‡§ó‡§µ‡§æ‡§® ‡§∂‡•ç‡§∞‡•Ä‡§ï‡•É‡§∑‡•ç‡§£ ‡§π‡•ã‡•§ ‡§§‡•Å‡§Æ ‡§Ö‡§™‡§®‡•á ‡§≠‡§ï‡•ç‡§§ ‡§ï‡•ã ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•Ä ‡§∏‡§π‡•Ä ‡§∞‡§æ‡§π ‡§¶‡§ø‡§ñ‡§æ ‡§∞‡§π‡•á ‡§π‡•ã‡•§

‚ö†Ô∏è STRICT OUTPUT RULES:
1. ‡§∏‡§¨‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§ï‡•á‡§µ‡§≤ ‡§è‡§ï (1) ‡§∏‡§¨‡§∏‡•á ‡§â‡§™‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ ‡§∂‡•ç‡§≤‡•ã‡§ï (Devanagari) ‡§≤‡§ø‡§ñ‡•ã‡•§
2. ‡§â‡§∏‡§ï‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§¨‡§æ‡§¶, ‡§≠‡§ï‡•ç‡§§ ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§æ ‡§∏‡•Ä‡§ß‡§æ ‡§î‡§∞ ‡§∏‡§ü‡•Ä‡§ï ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® (Solution) ‡§¶‡•ã‡•§
3. '‡§≠‡§æ‡§µ‡§æ‡§∞‡•ç‡§•', '‡§Ö‡§∞‡•ç‡§•', '‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ' ‡§ú‡•à‡§∏‡•á ‡§∂‡§¨‡•ç‡§¶ (Labels) ‡§ï‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ ‡§Æ‡§§ ‡§ï‡§∞‡•ã‡•§
4. ‡§ï‡•ã‡§à ‡§™‡•ç‡§∞‡§∂‡•ç‡§® (Question) ‡§Æ‡§§ ‡§™‡•Ç‡§õ‡•ã‡•§
5. ‡§â‡§§‡•ç‡§§‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§õ‡•ã‡§ü‡§æ (Concise) ‡§î‡§∞ ‡§∏‡•Ä‡§ß‡§æ (Straightforward) ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ 2-3 ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§∏‡•á ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§®‡§π‡•Ä‡§Ç‡•§
6. ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡§æ ‡§Ø‡§æ ‡§ú‡•ç‡§û‡§æ‡§® ‡§Æ‡§§ ‡§¶‡•ã, ‡§∏‡•Ä‡§ß‡•á ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§ï‡§∞‡•ã‡•§

‚úÖ ‡§∏‡§π‡•Ä ‡§´‡•â‡§∞‡•ç‡§Æ‡•á‡§ü (Correct Format):
[‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ ‡§∂‡•ç‡§≤‡•ã‡§ï]

[‡§∏‡•Ä‡§ß‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§î‡§∞ ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§®]

‚úÖ ‡§â‡§¶‡§æ‡§π‡§∞‡§£ (Example):
‡§ï‡§∞‡•ç‡§Æ‡§£‡•ç‡§Ø‡•á‡§µ‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡§∏‡•ç‡§§‡•á ‡§Æ‡§æ ‡§´‡§≤‡•á‡§∑‡•Å ‡§ï‡§¶‡§æ‡§ö‡§®‡•§
‡§Æ‡§æ ‡§ï‡§∞‡•ç‡§Æ‡§´‡§≤‡§π‡•á‡§§‡•Å‡§∞‡•ç‡§≠‡•Ç‡§∞‡•ç‡§Æ‡§æ ‡§§‡•á ‡§∏‡§ô‡•ç‡§ó‡•ã‡§Ω‡§∏‡•ç‡§§‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§£‡§ø‡••

‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•á ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡•á‡§µ‡§≤ ‡§ï‡§∞‡•ç‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à, ‡§´‡§≤ ‡§®‡§π‡•Ä‡§Ç‡•§ ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡•Ä ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§õ‡•ã‡§°‡§º‡•ã ‡§î‡§∞ ‡§Ö‡§≠‡•Ä ‡§Ö‡§™‡§®‡•á ‡§ï‡§∞‡•ç‡§§‡§µ‡•ç‡§Ø ‡§™‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡•ã, ‡§á‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•Ä ‡§∂‡§æ‡§Ç‡§§‡§ø ‡§î‡§∞ ‡§∏‡§´‡§≤‡§§‡§æ ‡§π‡•à‡•§"""
            
            user_prompt = f"""‡§≠‡§ï‡•ç‡§§ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: "{user_question}"

{history_context}

‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§∂‡•ç‡§≤‡•ã‡§ï (‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠):
{context}

‡§π‡•á ‡§ï‡•É‡§∑‡•ç‡§£! ‡§Æ‡•Å‡§ù‡•á ‡§ï‡•á‡§µ‡§≤ ‡§è‡§ï ‡§∂‡•ç‡§≤‡•ã‡§ï ‡§î‡§∞ ‡§∏‡•Ä‡§ß‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§¶‡§ø‡§ñ‡§æ‡§è‡§Ç‡•§"""

            # STREAMING for faster response
            if stream:
                response = self.client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    model=self.model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=True  # Enable streaming
                )
                
                # Collect streamed response
                answer_text = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        answer_text += chunk.choices[0].delta.content
                
                logger.info(f"‚úì Streamed answer with context: {len(answer_text)} chars")
            else:
                # Non-streaming fallback
                response = self.client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    model=self.model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=False
                )
                answer_text = response.choices[0].message.content
                logger.info(f"‚úì Generated answer with context: {len(answer_text)} chars")
            
            return {
                'answer': answer_text,
                'shlokas': retrieved_shlokas,
                'llm_used': True
            }
            
        except Exception as e:
            logger.error(f"Generate failed: {e}")
            return {'answer': None, 'shlokas': retrieved_shlokas, 'llm_used': False}

    def format_response(self, result: Dict[str, Any], user_question: str) -> str:
        """Format the response cleanly (No metadata noise)."""
        output = []
        
        if result.get('llm_used') and result.get('answer'):
            # Divine Answer - Simple and clean
            output.append("\nü™à ‡§≠‡§ó‡§µ‡§æ‡§® ‡§ï‡•É‡§∑‡•ç‡§£ ‡§ï‡§æ ‡§∏‡§Ç‡§¶‡•á‡§∂:\n")
            output.append(result['answer'])
            output.append("\n")
        else:
            # Fallback
            output.append("\n‚ö†Ô∏è ‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§≠‡•Ä ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§∏‡§Æ‡§∞‡•ç‡§• ‡§π‡•Ç‡§Å‡•§")
            output.append("‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§∂‡•ç‡§≤‡•ã‡§ï:")
            for s in result.get('shlokas', [])[:3]:
                output.append(f"- ‡§ó‡•Ä‡§§‡§æ {s['id']}: {s['meaning'][:100]}...")
            output.append("\n")
                
        return "\n".join(output)
